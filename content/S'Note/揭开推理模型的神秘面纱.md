---
tags: 
title: 揭开推理模型的神秘面纱
date: 2025-03-23T22:08:00
modified:
---
在《Demystifying Reasoning Models》一文中，作者Cameron R. Wolfe博士深入探讨了推理模型（Reasoning Models）在大型语言模型（LLMs）中的作用和机制。​推理模型在问题解决过程中，与传统的LLMs有着显著的区别，特别是在推理时间的分配和内部自我反思的能力方面。

**主要内容：**
1. *推理模型的定义与特点：
    - 推理模型通过在推理过程中进行自我反思和内部评估，来增强模型的推理能力和准确性。​
    - 这种方法类似于人类的元认知，即“思考自己的思考”，有助于处理需要多步骤推理和规划的复杂任务。
2. *推理模型的工作原理：*
    - 与传统的前馈神经网络不同，推理模型在推理过程中进行自我反思，识别并改正可能的错误，提高推理的准确性。​
    - 这种自我反思过程可以在推理的各个阶段进行，帮助模型更好地理解和解决复杂问题。
3. *推理模型的应用领域：*
    - **数学和逻辑推理：** 通过自我反思，模型能够解决需要多步骤推理的数学和逻辑问题。​
    - **视觉-语言任务：** 在视觉问答等任务中，模型通过反思图像内容和文本信息，提高理解和回答的准确性。
    - **一般问题解决：** 增强的反思能力有助于模型在规划、决策和创造性问题解决等方面表现更佳。​
4. *模型实例：*
    - **OpenAI的o3和o3-mini：** 这些模型结合了推理和搜索功能，能够执行复杂的多步骤推理任务。​
    - **DeepSeek的R1：** 通过强化学习和可验证奖励，DeepSeek-R1在推理任务中表现出色。​
    - **Gemini的Deep Research：** 将推理和搜索功能集成，支持多步骤研究任务的执行。​
5. *挑战与批评：*
    - **计算成本：** 推理模型需要更多的计算资源，可能导致成本增加。​
    - **延迟问题：** 反思过程可能增加响应时间，需要在性能和准确性之间找到平衡。​

过去几年，我们一直使用相对固定的流程来训练大型语言模型 (LLMs)。首先，我们利用来自互联网的原始文本数据对这些语言模型进行预训练。之后，我们通过监督微调 (SFT) 和从人类反馈中强化学习 (RLHF) 的组合对它们进行对齐 （或训练它们以产生人类更喜欢的输出 ） 。预训练和对齐在模型质量中都起着关键作用，但这一范式中的绝大部分进步都是由 LLM 缩放定律驱动的 —— 我们通过利用更多数据对更大的模型进行预训练来获得更好的结果 。
最近，一种全新的研究范式出现了： 推理 。与标准模型相比，推理模型以完全不同的方式解决问题。具体来说，它们在给出问题的最终答案之前会花费不同时间进行“思考”。训练能够有效思考的模型（如分解问题、检测思维中的错误、探索替代解决方案等）需要新的策略，通常涉及大规模强化学习 (RL)。此外，此类模型通过 RL 和推理产生了用于训练的新形式的缩放定律。![image.png](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323185032.png)
## 推理时代
随着推理模型的普及，我们见证了人工智能能力的突然显著提升 。首先发布的是 OpenAI 的 o1-preview，随后是一系列精简（即更小）的模型，如 o1-mini，及后来的模型变体，如 o3。作为回应，其他公司也发布了类似的推理模型，例如谷歌的 Gemini 2.0 Flash Thinking 。在本节中，我们将探讨这些最初的封闭式推理模型及其工作原理背后的基本思想。
##### 初始推理模型：o1 和 o1-mini
**长 CoT。** 推理模型与标准 LLM 的主要区别在于在回答问题之前“思考”的能力。推理模型的思维只是由 LLM 输出的长链思维（ 简称为长 CoT，有时称为推理轨迹或轨迹） 。此长 CoT 的生成方式与任何其他文本序列并无不同。然而，这些推理轨迹表现出非常有趣的特性，它们更类似于搜索算法，而不是原始文本生成。如：
- 仔细考虑复杂问题的每个部分。
- 将复杂问题分解为较小的、可解决的部分。
- 批评其自身的（部分）解决方案并发现错误。
- 探索多种替代解决方案。
（值得注意的是，OpenAI 推理模型使用的长 CoT 是“内部的”，这意味着在与模型交互时，它们对用户是隐藏的。相反，用户会看到模型编写的长 CoT 摘要。）

**推理能力。** 最初的推理模型实际上在许多方面都不如标准LLMs 1，但它们将LLM的推理能力提高了几个数量级。例如， o1-preview 的表现一致优于 GPT-4o，甚至可以与人类专家在大多数复杂推理任务上的表现相媲美。为了实现这些结果，o1-preview 用最大推理时间计算和单个输出样本（实线）或 64 个并行输出样本中的多数投票（阴影线）进行评估。
![image.png|575](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323190926.png)
![image.png|590](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323191019.png)
同样， o1-mini （ o1 的更便宜、更快版本） 具有令人印象深刻的推理能力，尽管与完整 o1 模型相比成本降低了 80%。与 o1 相比，o1-mini 的世界知识有限，但它在编码任务方面尤其出色，并且效率表现非常好。

##### 先进的推理模型：o3 和 o3-mini
o3 模型最初只是宣布（未发布）。我们能够在 OpenAI 测量的几个重要基准上看到该模型的性能 ，但无法实际使用该模型。OpenAI 发布的指标非常令人印象深刻。o3 最显著的成就是：
- GPT-4o 在 ARC-AGI 基准（AGI 的“北极星”）上的得分为 87.5% ， 五年来一直保持不败，其中 GPT-4o 实现了 5% 的准确率。o3 是第一个在 ARC-AGI 上超过人类 85% 水平表现的模型。
- 在 SWE-Bench Verified 上的准确率为 71.7% ， 在 Codeforces 上的 Elo 分数为 2727， 在全球排名前 200 位的竞技程序员中排名第 3。
- 在 EpochAI 的 FrontierMath 基准测试中的准确率为 25.2% ， 比之前最先进的 2.0% 的准确率有所提高。
然而，公众无法使用 o3 模型来验证这些结果。但 OpenAI 发布了该模型的较小版本 — o3-mini。

与 OpenAI 的其他推理模型相比，o3-mini 更具成本效益且更易于投入生产。该模型支持函数调用、Web 搜索和结构化输出等功能 。o3-mini 还具有多种设置， 包括低、中和高工作量 ，用于确定解决问题时执行的推理量。此设置可以直接在 API 请求中指定，并且该模型的表现非常出色--在许多情况下与 o1 相当 --具体取决于推理工作量的水平。
![image.png|650](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323192605.png)

**其他模型提供商**。
谷歌最近发布了实验性的 Gemini-2.0 Flash Thinking ，它保留了 Gemini 模型的标志性长上下文 --1M 令牌上下文窗口 --并在关键可验证任务（例如 AIME 和 GPQA）上取得了可观的指标。然而， 该模型的性能仍然落后于 o1 和 o3-mini 。
Grok-3 的推理测试版发布，非常引人注目。Grok-3 推理模型在高推理努力下超越了 o3-mini 的性能，甚至在某些情况下接近完整的 o3 模型；例如，AIME'24 的准确率为 96%，而 o3 的准确率为 97%。使用大规模新计算集群进行训练的 Grok-3 令人印象深刻（尤其是考虑到 xAI 还很年轻）。
![image.png|440](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323193322.png)
![image.png|380](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323212954.png)
![image.png|380](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323213022.png)

## 推理模型基础
上面提出的推理模型显然令人印象深刻，但它们都是封闭模型。因此， 我们不知道它们实际上如何工作 。
##### 可验证奖励的强化学习
**验证复杂性**。 根据我们正在解决的问题，验证LLM的输出可能会变得非常复杂。即使对于数学问题，验证LLM的答案与基本事实之间的匹配也很困难。例如，解决方案可能以不同的形式或格式呈现，从而导致假阴性验证。在这些情况下，简单的字符串匹配可能还不够！相反，我们可以提示LLM告诉我们这两个解决方案是否匹配，这可以大大减少错误验证。对于代码，实现验证也很困难--它需要构建一个数据管道，可以有效地在我们的训练设置中执行和验证测试案例 。

**神经验证。** 除了可验证问题之外，我们还可以考虑较弱的验证形式。例如，创意写作是一项难以验证的任务。可以：
- 训练神经奖励模型或验证器。
- 使用此模型对我们的LLM的输出进行评分。
- 使用预测分数作为奖励或验证信号。
这种设置与基于人类反馈的强化学习（RLHF）非常相似 。在这种情况下，我们正在训练我们的奖励模型，以根据模型响应的正确性或质量执行二进制验证。然而，使用神经验证器存在奖励黑客的风险 ，尤其是在执行大规模 RL 时。模型的训练时间更长，对奖励landscape的探索更多，从而增加了奖励黑客的风险。因此，许多最近的推理模型都避免了这种方法。

**从可验证的奖励中学习**。 我们现在了解了验证，但如何使用验证来训练LLM？很简单： 我们直接将验证结果用作使用 RL 进行训练的奖励信号。实现这个想法的方法有很多种（例如， 过程奖励或纯 RL ），但它们的共同主题是使用 RL 从可验证的奖励中学习。 这是所有现代推理模型所基于的基本概念 。
![image.png|530](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323195221.png)

##### 推理时间策略：思路链和解码
我们可以通过两种基本方法来增加语言模型在推理时消耗的计算量：
- 生成更多标记（即更长的输出序列）。
- 生成多个输出。
**思路链。** 推理模型使用的长 CoT 与标准 CoT 很不同。标准 CoT 简洁易懂，长 CoT 有几千个 token。虽然长 CoT 可以解释目的，但它并没有针对人类可读性进行优化。相反，它是一种广泛的推理轨迹，以详细的方式解决问题，并包含各种复杂的推理行为（例如回溯和自我改进）。
此外，推理模型在逻辑上将其 CoT 与模型的最终输出分开。如OpenAI 避免直接向用户展示较长的 CoT，而是提供生成的摘要来补充推理模型的最终答案。由于 CoT 很长，这种逻辑分离从根本上是必要的。
**并行解码**。 为了提高LLM最终输出的准确性，我们还可以使用并行解码技术。我们不是用LLM生成单个输出，而是生成多个输出并聚合这些输出以形成单个最终答案 。这种聚合可以通过多种方式完成；例如，使用多数投票或共识，使用加权投票 ，使用神经奖励模型或验证器 （即 Best-of-N 或拒绝抽样 ）或其他特定于领域的算法来识别最佳输出 。
我们还可以将拒绝抽样的思想应用于训练（即训练与测试时间拒绝抽样）。只需：
- 对多个输出或轨迹进行采样。
- 使用我们的奖励模型（或其他评分机制）来挑选最佳输出。
- 针对这些输出进行训练。
这种方法在实践中很常用；如LLaMA 模型在应用 RLHF 之前，在训练后过程中执行了几轮训练时拒绝采样。与基于 PPO 的 RLHF 相比，拒绝采样在实践中非常有效，且更容易实现和扩展 。
**自我完善。** 我们还可以考虑解码的批评或自我完善策略。首先，LLM 生成初始响应。然后，为响应提供反馈（ 来自 LLM 或某些外部来源），并且 LLM 可以根据反馈修改其响应。此循环可以重复任意次数。
存在几种不同的细化方法，但大致可以分为两类：
- 外部 ：反馈来自某些外部验证器或模块。
- 内在 ：LLM对其自身的生成提供反馈。
![image.png|335](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323200523.png)

## 开放推理：DeepSeek-R1 及更多
DeepSeek-R1 背后的核心思想与我们迄今为止所学到的知 识非常吻合。该模型使用 RL 在可验证任务上进行训练，在其中学习利用长 CoT 来解决复杂的推理问题。

“DeepSeek-R1-Zero 是一种通过大规模强化学习 (RL) 训练的模型，无需监督微调 (SFT) 作为初步步骤，它展示了非凡的推理能力。通过 RL，DeepSeek-R1-Zero 自然而然地呈现出许多强大而有趣的推理行为。”
![image.png](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323203503.png)

DeepSeek -R1-Zero 和 DeepSeek-R1 的创建都始于一个强大的基础模型（DeepSeek-v3，是一个 6710 亿参数混合专家- MoE 模型）。除了具有开放权重和详细的技术报告之外，该模型还超越了之前开放的 LLMs 的性能，甚至与封闭模型的质量相当。

##### DeepSeek-R1
DeepSeek-R1 经历四个训练阶段，包括两个 SFT 阶段和两个 RL 阶段。SFT 阶段的目的是在每个 RL 阶段为探索提供更好的起点。此训练流程提供了一种有效的方法，可将推理式训练与 LLMs 的标准训练方法相结合，让我们深入了解 DeepSeek-R1 使用的训练方法的每个阶段。

**第一阶段：冷启动（或面向推理的 SFT）**。 在进行 RL 训练之前，R1 通过 SFT 在一小组长 CoT 示例上进行训练。收集冷启动数据方法：
- 提示模型（例如 DeepSeek-v3）生成长 CoT 数据，可以使用少量示例，也可以指示模型生成详细答案并伴随反思和验证。
- 使用 R1-Zero 模型生成大量长 CoT 输出，然后要求人类进行后期处理并选择模型的最佳输出。

**第二阶段：以推理为导向的强化学习。** 在 SFT 之后，我们只是重复 R1-Zero 提出的大规模强化学习训练过程，以增强底层模型处理推理密集型任务的能力。DeepSeek-R1 的唯一变化是增加了语言一致性奖励，以模型输出中用目标语言编写的部分计算。这种语言一致性奖励会略微降低模型的推理能力。但是，语言一致性提高了最终模型与人类偏好的整体一致性--模型的输出更加流畅和可读。

**第三阶段：拒绝抽样**。 在面向推理的强化学习收敛之后，我们使用生成的模型来收集大量多样化的 SFT 数据集。然而，与最初的冷启动 SFT 阶段不同，我们收集的不仅仅是面向推理的数据，我们用通用数据来增强推理数据，以便模型可以从更广泛的问题和领域中学习。
在这个阶段，我们依赖的不仅仅是基于规则的技术来进行验证。我们还通过使用 DeepSeek-v3 作为生成奖励模型或弱验证器来整合来自不可验证域的额外数据。在应用启发式过滤（例如，删除带有语言混合或长段落的输出）后，我们最终得到了一组 600K 推理轨迹。
此阶段的 SFT 数据集包含大量非推理数据（例如写作或翻译示例）。我们从用于 DeepSeek-v3 的相同训练后数据集获取此数据。但是，通过要求 DeepSeek-v3 生成较长的 CoT 来解释复杂查询的输出，数据得到了增强 — 但是，较简单的查询没有任何 CoT 。总共收集了 200K 个非推理示例，形成了包含 800K 个示例的 SFT 数据集。

**第四阶段：通用 RLHF**。DeepSeek -R1 的最后训练阶段使模型与人类偏好保持一致，同时继续磨练其推理能力。与前一阶段类似，我们通过基于推理的数据和通用数据的组合来训练模型。具体来说，我们使用 RL 来训练模型，并为每种类型的数据组合不同的奖励：
- 基于规则的奖励（与 R1-Zero 相同），用于解决基于推理的问题。
- 神经奖励模型--根据人类偏好对进行训练，就像标准 RLHF 一样--用于通用数据。
DeepSeek-R1 经过校准，在通用数据上更有帮助且无害。研究中使用的两个非常常见的校准标准，每个标准都使用单独的神经奖励模型进行建模，模型在人类偏好的（监督）数据集上进行训练。有用性奖励仅根据模型的最终答案进行衡量（即不包括长 CoT），而无害奖励则考虑模型的整个输出轨迹。通过结合规则和基于偏好的奖励，DeepSeek-R1 可以与人类偏好保持一致，同时保持强大的推理性能。
![image.png](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323203058.png)

##### 蒸馏模型
除了 DeepSeek-R1，还发布了一系列从 R1 中提炼出来的密集模型。研究发现， 提炼过程可以显著增强小型高效模型的推理能力，它们与 R1 相当，但成本更经济且更易于使用 。此外，这些提炼模型的发布符合封闭推理模型（例如 o1-mini 和 o3-mini）的最新趋势。
![image.png](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323204415.png)
**提炼 R1**。 为了创建这些模型，我们从两个基础模型开始--Qwen-2.5 和 LLaMA-3。然后，通过 SFT 对 DeepSeek-R1 训练流程第三阶段精选的 800,000 个监督训练示例进行基础模型训练。
如上图示，蒸馏后的 Qwen2.5-14B 模型的表现优于 QwQ-32B-Preview ，后者是 R1 发布之前最好的开放式推理模型。此外，即使是最小的蒸馏模型也比未针对推理进行优化的标准封闭式 LLMs（例如 GPT-4o）表现更好，而 320 亿和 700 亿参数蒸馏模型在大多数基准测试中都超过了 o1-mini 的性能。
**蒸馏与强化学习**。 尽管蒸馏是有效的，但是否可以通过DeepSeek-R1 使用的大规模强化学习训练过程直接应用于这些较小的模型来获得更好的结果？有趣的是，使用上述蒸馏方法从 R1 中蒸馏出 Qwen2.5-32B 基础模型 ，其效果优于通过大规模强化学习直接训练该模型。
![image.png](https://raw.githubusercontent.com/SibylYang55/tuchuang/master/img/20250323205056.png)
**其他蒸馏推理模型。** 鉴于通过蒸馏训练高质量推理模型的简单性，研究社区根据 R1 发布了各种各样的推理模型。如：
- Sky-T1 and Sky-T1-Flash、Bespoke Stratos、LIMO、S1、RedStar

## 主要新兴趋势
现在已经了解了各种推理模型，从 o1 或 o3 等封闭模型开始，到 DeepSeek-R1 中对这些模型的完整复制结束。随着我们对这项研究的了解，我们开始发现一些共同的趋势。

**长 CoT（和推理时间扩展）。** 
推理模型和标准 LLMs 之间的关键区别在于它们的输出结构。推理模型不是直接生成最终答案（带有可选的简洁解释），而是生成一个长 CoT，详细描述其推理过程。这个长 CoT 的长度可变，从而实现推理时可控的计算成本： 更长的 CoT = 更多标记 = 更多计算。通过这种方式，在推理时使用更多计算（ 通过生成更长的 CoT）已成为一种工具，可让用户动态改进模型的推理能力。

**通过强化学习实现自我进化**。 
显然，LLMs 能够在较长的 CoT 内执行复杂的推理策略，这令人兴奋。发展这些特殊能力的关键因素是大规模强化学习训练，如果模型得到正确的激励，这种推理能力就会在强化学习过程中自然出现，通常是通过确定性和可靠的基于规则的奖励。此外，我们可以对「通过强化学习进行训练」使用更多计算来进一步提高模型的推理能力。

**监督较少。** 
与标准模型相比，推理模型对人类监督的依赖程度较低。具体来说，强化学习训练期间的奖励主要来自基于规则的系统，而不是依赖于人类偏好。当然，推理模型仍然有几个领域依赖于人类监督（如基础模型使用人类整理的数据进行训练，验证依赖于人类提供的基本事实标签。）；然而，像 R1（尤其是 R1-Zero）这样的推理模型仍然在大力推动，以证明推理能力可以自主发展。

**提炼是有效的。** 
现在我们可以使用大型且强大的推理模型，我们可以使用简单的策略将这些模型的功能提炼成更小、更密集的模型！这一发现引发了该领域的研究爆炸式增长，我们很可能会在不久的将来看到更多高效且提炼的推理模型发布。该领域的一个关键问题是较小的模型是否会泛化或难以完全匹配其原型的广度。

**需要解决的新问题。** 
最重要的是，推理模型的出现提出了各种新的问题，需要解决：
- 如何处理长期 CoT 的安全培训？
- 一般能力/推理能力之间的最佳平衡是什么？
- SFT 在训练推理模型中的最佳作用是什么？
- 如何最大限度地减少长期 CoT 中的“过度思考”？
- 如何处理推理模型的有效承载？